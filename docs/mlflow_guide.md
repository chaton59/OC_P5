# üöÄ Guide MLflow - Projet Employee Turnover

## üìã Table des mati√®res
1. [Workflow complet MLflow](#workflow-complet)
2. [Comparer plusieurs runs](#comparer-runs)
3. [Trouver le meilleur mod√®le](#meilleur-mod√®le)
4. [Model Registry](#model-registry)
5. [Best Practices](#best-practices)

---

## 1. Workflow complet MLflow

### üéØ Concept cl√©
MLflow suit ce workflow :
```
Entra√Ænement ‚Üí Tracking ‚Üí Registry ‚Üí S√©lection du meilleur mod√®le
```

### Architecture actuelle du projet
```
train_model.py
    ‚Üì (log params/metrics/model)
mlflow.db (SQLite)
    ‚Üì (query)
MLflow UI (http://localhost:5000)
    ‚Üì (select best model)
Model Registry (XGBoost_Employee_Turnover)
    ‚Üì (versions & stages)
Mod√®le pr√™t pour d√©ploiement
```

---

## 2. Comparer plusieurs runs

### Sc√©nario : Tester diff√©rents hyperparam√®tres

**Exemple : Tester 3 configurations diff√©rentes**

```python
# tests/test_multiple_runs.py
import mlflow
from ml_model.preprocess import preprocess_data
from ml_model.train_model import train_model

mlflow.set_tracking_uri("sqlite:///mlflow.db")
mlflow.set_experiment("Hyperparameter_Tuning")  # Cr√©er une exp√©rience d√©di√©e

# Chemins des donn√©es
data_paths = {
    "sondage_path": "data/extrait_sondage.csv",
    "eval_path": "data/extrait_eval.csv",
    "sirh_path": "data/extrait_sirh.csv",
}

# Pr√©parer les donn√©es une seule fois
X, y, scaler, encoders = preprocess_data(data_paths)

# Tester 3 configurations
configs = [
    {"name": "Baseline", "n_iter": 100, "cv": 3},
    {"name": "Intensive", "n_iter": 500, "cv": 5},
    {"name": "Quick", "n_iter": 50, "cv": 3},
]

for config in configs:
    with mlflow.start_run(run_name=config["name"]):
        # Log la configuration test√©e
        mlflow.log_param("config_name", config["name"])
        mlflow.log_param("n_iter", config["n_iter"])
        mlflow.log_param("cv", config["cv"])
        
        # Entra√Æner (modifier train_model pour accepter n_iter/cv)
        model, params, cv_f1 = train_model(X, y)
        
        print(f"‚úÖ {config['name']}: F1={cv_f1:.4f}")
```

**R√©sultat dans MLflow UI** :
- Va sur **Experiments** ‚Üí **Hyperparameter_Tuning**
- Tu verras 3 runs avec leurs m√©triques c√¥te √† c√¥te
- Clique sur **"Compare"** pour voir un tableau comparatif

---

## 3. Trouver le meilleur mod√®le

### Via l'API MLflow

```python
# examples/find_best_model.py (d√©j√† cr√©√© dans le projet)
import mlflow
from mlflow.tracking import MlflowClient

mlflow.set_tracking_uri("sqlite:///mlflow.db")
client = MlflowClient()

def get_best_model_from_experiment(experiment_name="Default", metric="cv_f1"):
    """
    Trouve le meilleur mod√®le d'une exp√©rience bas√© sur une m√©trique.
    
    Args:
        experiment_name: Nom de l'exp√©rience MLflow
        metric: M√©trique √† optimiser (cv_f1, test_f1, etc.)
    
    Returns:
        run_id du meilleur mod√®le
    """
    # R√©cup√©rer l'exp√©rience
    experiment = client.get_experiment_by_name(experiment_name)
    if not experiment:
        raise ValueError(f"Exp√©rience '{experiment_name}' introuvable")
    
    # Rechercher tous les runs de l'exp√©rience
    runs = client.search_runs(
        experiment_ids=[experiment.experiment_id],
        order_by=[f"metrics.{metric} DESC"],
        max_results=1
    )
    
    if not runs:
        raise ValueError(f"Aucun run trouv√© dans l'exp√©rience '{experiment_name}'")
    
    best_run = runs[0]
    print(f"üèÜ Meilleur mod√®le trouv√©:")
    print(f"   Run ID: {best_run.info.run_id}")
    print(f"   {metric}: {best_run.data.metrics.get(metric, 'N/A')}")
    
    return best_run.info.run_id

# Charger le mod√®le
best_run_id = get_best_model_from_experiment("Default", "cv_f1")
model_uri = f"runs:/{best_run_id}/model"
model = mlflow.sklearn.load_model(model_uri)
```

---

## 4. Model Registry

### G√©rer les versions de mod√®les

```python
# examples/model_registry.py (d√©j√† cr√©√© dans le projet)
from mlflow.tracking import MlflowClient

client = MlflowClient()
model_name = "XGBoost_Employee_Turnover"

# Lister les versions
versions = client.search_model_versions(f"name='{model_name}'")
for v in versions:
    print(f"Version {v.version}: {v.current_stage}")

# Promouvoir en Production
client.transition_model_version_stage(
    name=model_name,
    version=1,
    stage="Production",
    archive_existing_versions=True
)

# Charger depuis le Registry
model = mlflow.sklearn.load_model(f"models:/{model_name}/Production")
```

---

## 5. Best Practices

### ‚úÖ Strat√©gie de versioning des mod√®les

```python
# Workflow recommand√©
# 1. Entra√Æner plusieurs mod√®les ‚Üí Experiment "Development"
# 2. S√©lectionner le meilleur ‚Üí Promouvoir en "Staging"
# 3. Valider en staging ‚Üí Promouvoir en "Production"

from mlflow.tracking import MlflowClient

client = MlflowClient()
model_name = "XGBoost_Employee_Turnover"

# Promouvoir version 2 en Production
client.transition_model_version_stage(
    name=model_name,
    version=2,
    stage="Production"
)
```

### üìä Logging avanc√©

```python
# Dans train_model.py, ajouter plus de contexte
with mlflow.start_run():
    # Log dataset info
    mlflow.log_param("n_samples", len(X))
    mlflow.log_param("n_features", X.shape[1])
    mlflow.log_param("class_imbalance_ratio", sum(y==0)/sum(y==1))
    
    # Log artifacts (graphiques)
    import matplotlib.pyplot as plt
    plt.figure()
    # ... plot code ...
    plt.savefig("confusion_matrix.png")
    mlflow.log_artifact("confusion_matrix.png")
    
    # Log code version
    import subprocess
    git_commit = subprocess.check_output(['git', 'rev-parse', 'HEAD']).strip().decode()
    mlflow.set_tag("git_commit", git_commit)
```

### üîÑ Retraining workflow

```python
# scripts/retrain_model.py
import mlflow

def retrain_and_compare():
    """Entra√Æne un nouveau mod√®le et le compare √† la production."""
    
    # 1. Charger le mod√®le en production
    prod_model = mlflow.sklearn.load_model("models:/XGBoost_Employee_Turnover/Production")
    
    # 2. Entra√Æner nouveau mod√®le
    X, y, _, _ = preprocess_data(data_paths)
    new_model, params, new_f1 = train_model(X, y)
    
    # 3. Comparer les performances
    from sklearn.model_selection import cross_val_score
    prod_f1 = cross_val_score(prod_model, X, y, cv=5, scoring='f1').mean()
    
    print(f"Production F1: {prod_f1:.4f}")
    print(f"New model F1: {new_f1:.4f}")
    
    # 4. Si meilleur, promouvoir automatiquement
    if new_f1 > prod_f1:
        print("‚úÖ Nouveau mod√®le meilleur ! Promotion en Staging...")
    else:
        print("‚ö†Ô∏è Nouveau mod√®le moins bon, conservation du mod√®le actuel")
```

---

## üìö Ressources

- **MLflow Docs**: https://mlflow.org/docs/latest/index.html
- **Model Registry**: https://mlflow.org/docs/latest/model-registry.html
- **Python API**: https://mlflow.org/docs/latest/python_api/index.html

---

## üéØ Utilisation du projet

### Entra√Æner un mod√®le
```bash
python ml_model/train_model.py
```

### Lancer MLflow UI
```bash
mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5000
```

### Exemples disponibles
```bash
# Trouver le meilleur mod√®le
python examples/01_find_best_model.py

# Comparer tous les runs
python examples/02_compare_models.py

# G√©rer le Model Registry
python examples/03_model_registry.py
```
