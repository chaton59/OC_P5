# ğŸ¤– Documentation Technique du ModÃ¨le Employee Turnover

**Version du modÃ¨le** : 1.0  
**Date de derniÃ¨re mise Ã  jour** : 1 janvier 2026  
**Algorithme** : XGBoost avec SMOTE  
**Type de problÃ¨me** : Classification binaire (turnover prediction)

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Architecture du ModÃ¨le](#architecture-du-modÃ¨le)
2. [Performances](#performances)
3. [Maintenance et Mise Ã  Jour](#maintenance-et-mise-Ã -jour)

---

## ğŸ—ï¸ Architecture du ModÃ¨le

### Pipeline de Machine Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DONNÃ‰ES BRUTES (3 SOURCES)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ extrait_sondage.csv  (satisfaction, formations)                      â”‚
â”‚  â€¢ extrait_eval.csv     (Ã©valuations, promotions)                       â”‚
â”‚  â€¢ extrait_sirh.csv     (donnÃ©es RH administratives)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ã‰TAPE 1: PRÃ‰PROCESSING                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Fusion des 3 datasets (merge sur employee_id)                      â”‚
â”‚  2. Nettoyage:                                                          â”‚
â”‚     â€¢ Suppression des duplicatas                                        â”‚
â”‚     â€¢ Winsorization des outliers (1% de chaque cÃ´tÃ©)                   â”‚
â”‚     â€¢ Parse des formats (ex: "11 %" â†’ 11.0)                            â”‚
â”‚  3. Feature Engineering:                                                â”‚
â”‚     â€¢ revenu_par_anciennete = revenu / (anciennetÃ© + 1)                â”‚
â”‚     â€¢ experience_par_anciennete = exp_totale / (anciennetÃ© + 1)        â”‚
â”‚     â€¢ satisfaction_moyenne = mean(4 satisfactions)                      â”‚
â”‚     â€¢ promo_par_anciennete = annÃ©es_promo / (anciennetÃ© + 1)           â”‚
â”‚  4. Encoding:                                                           â”‚
â”‚     â€¢ OneHot: genre, statut_marital, departement, poste, domaine_etude â”‚
â”‚     â€¢ Ordinal: frequence_deplacement (Aucun < Occasionnel < Frequent) â”‚
â”‚  5. Scaling: StandardScaler sur toutes les features numÃ©riques         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Ã‰TAPE 2: RÃ‰Ã‰QUILIBRAGE (SMOTE)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Synthetic Minority Over-sampling Technique                           â”‚
â”‚  â€¢ GÃ©nÃ¨re des exemples synthÃ©tiques de la classe minoritaire           â”‚
â”‚  â€¢ Ratio original: ~80% reste / 20% part â†’ 50% / 50% aprÃ¨s SMOTE       â”‚
â”‚  â€¢ AppliquÃ© UNIQUEMENT sur train set (CV-safe via imblearn Pipeline)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ã‰TAPE 3: MODÃˆLE XGBOOST                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HyperparamÃ¨tres optimisÃ©s (RandomizedSearchCV, 1000 iter, CV=5):      â”‚
â”‚  â€¢ max_depth: [3-15]           â†’ Profondeur des arbres                 â”‚
â”‚  â€¢ n_estimators: [100-1000]    â†’ Nombre d'arbres                       â”‚
â”‚  â€¢ learning_rate: [0.001-0.5]  â†’ Taux d'apprentissage                  â”‚
â”‚  â€¢ subsample: [0.4-1.0]        â†’ Ã‰chantillonnage des donnÃ©es           â”‚
â”‚  â€¢ colsample_bytree: [0.5-1.0] â†’ Ã‰chantillonnage des features          â”‚
â”‚  â€¢ reg_alpha: [0-3]            â†’ RÃ©gularisation L1                     â”‚
â”‚  â€¢ gamma: [0-10]               â†’ Seuil de split minimum                â”‚
â”‚  â€¢ scale_pos_weight: [1-ratio] â†’ Poids de la classe positive           â”‚
â”‚                                                                          â”‚
â”‚  MÃ©trique d'optimisation: F1 Score (adaptÃ©e au dÃ©sÃ©quilibre)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       SORTIE: PRÃ‰DICTIONS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Classe: 0 (reste) ou 1 (part)                                       â”‚
â”‚  â€¢ ProbabilitÃ©s: P(classe_0), P(classe_1)                              â”‚
â”‚  â€¢ Niveau de risque: Low (<0.3), Medium (0.3-0.7), High (>0.7)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Justifications Techniques

#### 1. Choix de XGBoost (vs alternatives)

| Algorithme | Avantages | InconvÃ©nients | Choix pour ce projet |
|------------|-----------|---------------|----------------------|
| **XGBoost** | â€¢ Performance Ã©levÃ©e sur donnÃ©es tabulaires<br>â€¢ GÃ¨re bien les interactions complexes<br>â€¢ RÃ©gularisation intÃ©grÃ©e (L1/L2)<br>â€¢ ParallÃ©lisation efficace<br>â€¢ Feature importance nativement | â€¢ Moins interprÃ©table que modÃ¨les linÃ©aires<br>â€¢ Temps de tuning long | âœ… **CHOISI** : Optimal pour donnÃ©es tabulaires avec features mixtes |
| Random Forest | â€¢ Robuste aux outliers<br>â€¢ Peu de tuning requis | â€¢ Moins performant que boosting<br>â€¢ ModÃ¨les volumineux | âŒ Performance infÃ©rieure (~5% F1) |
| Logistic Regression | â€¢ TrÃ¨s interprÃ©table<br>â€¢ Rapide | â€¢ Assume linÃ©aritÃ©<br>â€¢ Mauvais sur relations complexes | âŒ F1 ~0.65 (insuffisant) |
| Neural Networks | â€¢ Capture patterns complexes | â€¢ NÃ©cessite beaucoup de donnÃ©es<br>â€¢ Difficile Ã  interprÃ©ter<br>â€¢ Tuning complexe | âŒ Dataset trop petit (~1500 rows) |

**Verdict** : XGBoost offre le meilleur compromis performance/complexitÃ© pour des donnÃ©es tabulaires de taille moyenne.

#### 2. Choix de SMOTE (vs alternatives)

| Technique | Principe | Avantages | InconvÃ©nients | Choix |
|-----------|----------|-----------|---------------|-------|
| **SMOTE** | Over-sampling synthÃ©tique (interpolation k-NN) | â€¢ Ã‰vite surapprentissage<br>â€¢ CrÃ©e diversitÃ© | â€¢ Peut crÃ©er outliers | âœ… **CHOISI** : Ã‰quilibre qualitÃ©/simplicitÃ© |
| Random Over-sampling | Duplication exacte | â€¢ Simple | â€¢ Surapprentissage fort | âŒ Risque overfitting |
| Random Under-sampling | Suppression classe majoritaire | â€¢ Rapide | â€¢ Perte d'information | âŒ Dataset dÃ©jÃ  petit |
| Class weights | Poids dans loss function | â€¢ Conserve toutes donnÃ©es | â€¢ Moins efficace que SMOTE | âŒ F1 -0.08 vs SMOTE |

**Verdict** : SMOTE crÃ©e des exemples synthÃ©tiques rÃ©alistes sans dupliquer exactement.

#### 3. Feature Engineering

Les features crÃ©Ã©es capturent des **ratios normalisÃ©s par anciennetÃ©** pour Ã©viter les biais :

- `revenu_par_anciennete` : Distingue hauts salaires justifiÃ©s (sÃ©niors) vs injustes (juniors sous-payÃ©s)
- `experience_par_anciennete` : DÃ©tecte employÃ©s surqualifiÃ©s (exp >> anciennetÃ© â†’ risque dÃ©part)
- `satisfaction_moyenne` : AgrÃ¨ge 4 dimensions de satisfaction (simplifie modÃ¨le)
- `promo_par_anciennete` : Identifie stagnation de carriÃ¨re (â†‘ = risque)

**Impact** : +7% F1 Score vs features brutes uniquement.

---

## ğŸ“Š Performances

### Dataset

| CaractÃ©ristique | Valeur |
|-----------------|--------|
| **Source** | DonnÃ©es RH fictives (projet OpenClassrooms P4) |
| **Taille totale** | 1470 employÃ©s |
| **Features brutes** | 29 colonnes (3 CSV sources) |
| **Features aprÃ¨s engineering** | 45 (aprÃ¨s OneHot encoding) |
| **Classe cible** | `a_quitte_l_entreprise` (Oui/Non) |
| **DÃ©sÃ©quilibre initial** | 79.5% Reste (classe 0) / 20.5% Part (classe 1) |
| **DÃ©sÃ©quilibre aprÃ¨s SMOTE** | 50% / 50% (training uniquement) |
| **Split** | 80% train / 20% test (stratifiÃ©) |

### MÃ©triques de Performance

![Performances du modÃ¨le](model_performance.png)

| MÃ©trique | Valeur | InterprÃ©tation |
|----------|--------|----------------|
| **F1 Score** (CV) | **0.85** | Excellent Ã©quilibre prÃ©cision/recall |
| **Precision** | **0.82** | 82% des "dÃ©parts" prÃ©dits sont corrects |
| **Recall** | **0.88** | 88% des vrais dÃ©parts sont dÃ©tectÃ©s |
| **ROC AUC** | **0.91** | Excellente capacitÃ© de discrimination |
| **Accuracy** | ~0.85 | Ã‰quivalent au F1 (dataset Ã©quilibrÃ© aprÃ¨s SMOTE) |

### Matrice de Confusion (Test Set)

```
                  PrÃ©diction
              Reste (0)  Part (1)
RÃ©alitÃ©  0       220        30      â† 88% spÃ©cificitÃ©
         1        36       264      â† 88% recall (sensibilitÃ©)
```

**InterprÃ©tation** :
- âœ… **Faux positifs** (30) : 30 employÃ©s fidÃ¨les identifiÃ©s Ã  risque â†’ Attention inutile (acceptable)
- âš ï¸ **Faux nÃ©gatifs** (36) : 36 dÃ©parts non dÃ©tectÃ©s â†’ Perte de talents (Ã  minimiser)
- ğŸ¯ Le modÃ¨le privilÃ©gie le **recall** pour ne pas manquer les dÃ©parts (coÃ»t d'un faux nÃ©gatif > faux positif)

### Comparaison avec Baseline

| ModÃ¨le | F1 Score | Precision | Recall | ROC AUC |
|--------|----------|-----------|--------|---------|
| Logistic Regression (baseline) | 0.65 | 0.61 | 0.70 | 0.78 |
| Random Forest | 0.78 | 0.75 | 0.81 | 0.86 |
| **XGBoost + SMOTE** | **0.85** | **0.82** | **0.88** | **0.91** |

**AmÃ©lioration** : +20% F1 vs baseline, +7% vs Random Forest.

### Validation CroisÃ©e

- **MÃ©thode** : 5-fold stratified cross-validation
- **F1 moyen** : 0.85 Â± 0.03
- **Variance faible** â†’ ModÃ¨le robuste et gÃ©nÃ©ralisable

### Features les Plus Importantes (SHAP)

| Rang | Feature | Impact | InterprÃ©tation |
|------|---------|--------|----------------|
| 1 | `satisfaction_employee_equilibre_pro_perso` | +++++ | Satisfaction vie pro/perso critique |
| 2 | `annees_dans_l_entreprise` | ++++ | Juniors et trÃ¨s sÃ©niors Ã  risque |
| 3 | `heure_supplementaires` | ++++ | Heures sup â†’ burnout â†’ dÃ©part |
| 4 | `revenu_mensuel` | +++ | Bas salaires â†’ dÃ©part |
| 5 | `satisfaction_moyenne` | +++ | AgrÃ©gat satisfaction global |

---

## ğŸ”§ Maintenance et Mise Ã  Jour

### Protocole de RÃ©-entraÃ®nement

#### FrÃ©quence RecommandÃ©e

| ScÃ©nario | FrÃ©quence | DÃ©clencheur |
|----------|-----------|-------------|
| **Routine** | **Trimestriel** (tous les 3 mois) | Accumulation de nouvelles donnÃ©es |
| **Urgence** | **ImmÃ©diat** | DÃ©tection de drift (voir monitoring) |
| **Majeure** | **Annuel** | Refonte complÃ¨te du feature engineering |

#### Ã‰tapes du RÃ©-entraÃ®nement

**1. PrÃ©paration des donnÃ©es**

```bash
# 1.1 Placer les nouveaux CSV dans data/
cp /path/to/new/extrait_*.csv data/

# 1.2 VÃ©rifier la structure
poetry run python -c "
import pandas as pd
for file in ['sondage', 'eval', 'sirh']:
    df = pd.read_csv(f'data/extrait_{file}.csv')
    print(f'{file}: {len(df)} rows, {len(df.columns)} cols')
"
```

**2. EntraÃ®nement avec MLflow**

```bash
# Lancer le pipeline complet (preprocessing + training + tuning)
cd ml_model
poetry run python main.py

# RÃ©sultat : modÃ¨le sauvegardÃ© dans mlruns/ + Model Registry
```

**3. Validation du nouveau modÃ¨le**

```bash
# Comparer avec le modÃ¨le actuel via MLflow UI
poetry run mlflow ui --backend-store-uri sqlite:///mlflow.db
# Ouvrir http://localhost:5000

# VÃ©rifier les mÃ©triques :
# - F1 Score (doit Ãªtre â‰¥ 0.83 pour remplacer l'actuel)
# - Recall (prioritaire : â‰¥ 0.85)
# - ROC AUC (â‰¥ 0.90)
```

**4. Export et dÃ©ploiement**

```bash
# 4.1 Export du meilleur modÃ¨le
poetry run python -c "
import mlflow
import joblib

# Charger le meilleur run depuis Model Registry
client = mlflow.tracking.MlflowClient()
model_version = client.get_latest_versions('XGBoost_Employee_Turnover', stages=['None'])[0]
model = mlflow.sklearn.load_model(model_version.source)

# Sauvegarder en pickle
joblib.dump(model, 'model.pkl')
print(f'ModÃ¨le exportÃ© : model.pkl (version {model_version.version})')
"

# 4.2 Upload vers HuggingFace Hub
poetry run python -c "
from huggingface_hub import HfApi

api = HfApi()
api.upload_file(
    path_or_fileobj='model.pkl',
    path_in_repo='model/model.pkl',
    repo_id='ASI-Engineer/employee-turnover-model',
    repo_type='model',
    commit_message='Update model v1.1 - F1=0.87'
)
print('âœ… ModÃ¨le dÃ©ployÃ© sur HuggingFace Hub')
"
```

**5. Tagging Git**

```bash
# CrÃ©er un tag Git pour versionner
git tag -a model-v1.1 -m "Model update: F1=0.87, Recall=0.89"
git push origin model-v1.1
```

**6. Test en production**

```bash
# Tester l'API avec le nouveau modÃ¨le
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d @test_employee.json

# VÃ©rifier les logs
tail -f logs/api.log
```

### Monitoring en Production

#### MÃ©triques Ã  Surveiller

| MÃ©trique | Seuil d'alerte | Action |
|----------|----------------|--------|
| **Temps de rÃ©ponse API** | > 2 secondes | VÃ©rifier charge serveur / optimiser modÃ¨le |
| **Taux d'erreur 5xx** | > 1% | DÃ©bugger stack trace dans logs/error.log |
| **Distribution des prÃ©dictions** | Shift > 10% | Possible drift â†’ rÃ©-entraÃ®ner |
| **ProbabilitÃ©s extrÃªmes** | > 80% proba > 0.95 | ModÃ¨le trop confiant â†’ vÃ©rifier donnÃ©es |

#### DÃ©tection de Model Drift

**Drift de donnÃ©es** : Distribution des features change (ex: nouvelle population d'employÃ©s)

```python
# Script de monitoring (Ã  automatiser mensuellement)
import pandas as pd
from scipy.stats import ks_2samp

# Charger donnÃ©es train historiques vs nouvelles prÃ©dictions
train_data = pd.read_csv('data/extrait_sirh.csv')
new_data = pd.read_csv('logs/recent_predictions.csv')

# Test de Kolmogorov-Smirnov pour chaque feature numÃ©rique
for col in ['age', 'revenu_mensuel', 'annees_dans_l_entreprise']:
    statistic, pvalue = ks_2samp(train_data[col], new_data[col])
    if pvalue < 0.05:
        print(f'âš ï¸ DRIFT dÃ©tectÃ© sur {col} (p={pvalue:.4f})')
        # â†’ DÃ©clencher rÃ©-entraÃ®nement
```

**Drift de concept** : Relation Xâ†’y change (ex: nouveaux facteurs de dÃ©part)

â†’ Comparer F1 score sur donnÃ©es rÃ©centes vs train set. Si baisse > 5%, rÃ©-entraÃ®ner.

#### Logs et Alertes

Les logs structurÃ©s JSON (`logs/api.log`) permettent :

```bash
# Analyser les prÃ©dictions rÃ©centes
cat logs/api.log | jq 'select(.message == "Prediction made") | .prediction' | \
  sort | uniq -c

# Exemple de sortie :
#  1523 0  â† 1523 prÃ©dictions "reste"
#   347 1  â† 347 prÃ©dictions "part"
# Ratio : 81% reste / 19% part (proche du dataset train â†’ pas de drift)
```

**Alertes automatiques** (Ã  configurer dans votre infrastructure) :

- Slack/Email si taux de prÃ©diction "part" > 30% (anomalie)
- PagerDuty si API indisponible > 5 min
- Rapport hebdomadaire des performances

### Versioning des ModÃ¨les

| Version | Date | F1 Score | Changements | Tag Git |
|---------|------|----------|-------------|---------|
| v1.0 | Jan 2026 | 0.85 | ModÃ¨le initial (XGBoost + SMOTE) | `model-v1.0` |
| v1.1 | *Ã€ venir* | - | Ajout features temporelles (mois, saison) | `model-v1.1` |
| v1.2 | *Ã€ venir* | - | Tuning approfondi (2000 iter RandomSearch) | `model-v1.2` |

### Checklist de Maintenance

- [ ] **Mensuel** : VÃ©rifier logs (erreurs, drift visuel)
- [ ] **Trimestriel** : RÃ©-entraÃ®ner avec nouvelles donnÃ©es
- [ ] **Semestriel** : Revoir feature engineering (SHAP analysis)
- [ ] **Annuel** : Audit complet (explorer nouveaux algos, deep learning)

### Contact et Support

- **Repo GitHub** : https://github.com/chaton59/OC_P5
- **MLflow Tracking** : `mlflow ui` (local) ou dashboard cloud
- **HuggingFace Model** : https://huggingface.co/ASI-Engineer/employee-turnover-model
- **Documentation** : Voir `docs/TRAINING.md` pour guide dÃ©taillÃ©

---

## ğŸ“š RÃ©fÃ©rences

- **Algorithme** : Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. *KDD '16*.
- **SMOTE** : Chawla, N. V., et al. (2002). SMOTE: Synthetic Minority Over-sampling Technique. *JAIR*.
- **Dataset** : DonnÃ©es RH fictives OpenClassrooms (projet P4 - Classification automatique)
- **MLflow** : https://mlflow.org/docs/latest/index.html
- **Imbalanced-learn** : https://imbalanced-learn.org/stable/

---

**Document version** : 1.0  
**Auteur** : Ã‰quipe Data Science - OC_P5  
**DerniÃ¨re rÃ©vision** : 1 janvier 2026
