# ğŸ¤– ModÃ¨le Machine Learning

Documentation technique du modÃ¨le Employee Turnover Prediction.

---

## ğŸ“Š Vue d'Ensemble

**Algorithme** : XGBoost avec SMOTE  
**Type** : Classification binaire (dÃ©part / reste)  
**Dataset** : 1470 employÃ©s, 29 variables  
**Performance** : F1 Score 0.85, Recall 88%

---

## ğŸ—ï¸ Architecture

### Pipeline Complet

```
DONNÃ‰ES BRUTES (3 CSV)
â”œâ”€â”€ extrait_sondage.csv
â”œâ”€â”€ extrait_eval.csv
â””â”€â”€ extrait_sirh.csv
    â”‚
    â–¼
PREPROCESSING
â”œâ”€â”€ Fusion sur employee_id
â”œâ”€â”€ Nettoyage (duplicatas, outliers)
â”œâ”€â”€ Feature Engineering (ratios, moyennes)
â”œâ”€â”€ Encoding (OneHot + Ordinal)
â””â”€â”€ Scaling (StandardScaler)
    â”‚
    â–¼
RÃ‰Ã‰QUILIBRAGE (SMOTE)
â”œâ”€â”€ 79% reste â†’ 50% reste
â””â”€â”€ 21% part â†’ 50% part
    â”‚
    â–¼
MODÃˆLE XGBOOST
â”œâ”€â”€ RandomizedSearchCV (1000 itÃ©rations)
â”œâ”€â”€ Cross-validation 5-fold
â””â”€â”€ Optimisation F1 Score
    â”‚
    â–¼
PRÃ‰DICTIONS
â”œâ”€â”€ Classe: 0 (reste) / 1 (part)
â”œâ”€â”€ ProbabilitÃ©s: [0.0 - 1.0]
â””â”€â”€ Niveau de risque: Low/Medium/High
```

---

## ğŸ”§ Preprocessing

### 1. Fusion des DonnÃ©es

Merge des 3 sources sur `employee_id` :
- `extrait_sondage.csv` : satisfaction, formations
- `extrait_eval.csv` : Ã©valuations, promotions
- `extrait_sirh.csv` : donnÃ©es administratives

### 2. Nettoyage

- **Duplicatas** : suppression
- **Outliers** : winsorization (1% chaque cÃ´tÃ©)
- **Parsing** : `"11 %"` â†’ `11.0`

### 3. Feature Engineering

| Feature CrÃ©Ã©e | Formule | Objectif |
|---------------|---------|----------|
| `revenu_par_anciennete` | revenu / (anciennetÃ© + 1) | DÃ©tecter sous-rÃ©munÃ©ration |
| `experience_par_anciennete` | exp_totale / (anciennetÃ© + 1) | Identifier surqualifiÃ©s |
| `satisfaction_moyenne` | mean(4 satisfactions) | AgrÃ©gat de satisfaction |
| `promo_par_anciennete` | annÃ©es_promo / (anciennetÃ© + 1) | DÃ©tecter stagnation |

**Impact** : +7% F1 Score

### 4. Encoding

**OneHot** (variables non-ordonnÃ©es) :
- `genre` : Homme / Femme
- `statut_marital` : MariÃ© / CÃ©libataire / DivorcÃ©
- `departement` : Commercial, RH, Recherche, IT, Ventes
- `poste` : 15 postes diffÃ©rents
- `domaine_etude` : 6 domaines

**Ordinal** (variable ordonnÃ©e) :
- `frequence_deplacement` : Aucun (0) < Occasionnel (1) < Frequent (2)

### 5. Scaling

**StandardScaler** sur toutes les features numÃ©riques :
- Moyenne = 0
- Ã‰cart-type = 1

---

## âš–ï¸ RÃ©Ã©quilibrage SMOTE

**ProblÃ¨me** : Dataset dÃ©sÃ©quilibrÃ© (79% reste / 21% part)  
**Solution** : SMOTE (Synthetic Minority Over-sampling Technique)

### Principe

CrÃ©e des exemples synthÃ©tiques de la classe minoritaire par interpolation k-NN.

### Application

- **Avant SMOTE** : 1176 reste (79%) / 294 part (21%)
- **AprÃ¨s SMOTE** : 1176 reste (50%) / 1176 part (50%)
- **AppliquÃ©** : Training set uniquement (CV-safe)

### Alternatives Ã‰cartÃ©es

| Technique | ProblÃ¨me |
|-----------|----------|
| Random Over-sampling | Surapprentissage (duplication exacte) |
| Random Under-sampling | Perte d'information |
| Class weights | -8% F1 vs SMOTE |

---

## ğŸ¯ ModÃ¨le XGBoost

### HyperparamÃ¨tres OptimisÃ©s

RandomizedSearchCV avec 1000 itÃ©rations, 5-fold CV :

| ParamÃ¨tre | Plage | Optimal | RÃ´le |
|-----------|-------|---------|------|
| `n_estimators` | 100-1000 | 300 | Nombre d'arbres |
| `max_depth` | 3-15 | 7 | Profondeur des arbres |
| `learning_rate` | 0.001-0.5 | 0.1 | Taux d'apprentissage |
| `subsample` | 0.4-1.0 | 0.8 | Ã‰chantillonnage donnÃ©es |
| `colsample_bytree` | 0.5-1.0 | 0.9 | Ã‰chantillonnage features |
| `reg_alpha` | 0-3 | 0.5 | RÃ©gularisation L1 |
| `gamma` | 0-10 | 2 | Seuil de split |

### Pourquoi XGBoost ?

| Algorithme | F1 Score | Avantage XGBoost |
|------------|----------|------------------|
| Logistic Regression | 0.65 | Capture relations non-linÃ©aires |
| Random Forest | 0.78 | +7% performance |
| **XGBoost** | **0.85** | RÃ©gularisation + Boosting |
| Neural Network | N/A | Dataset trop petit |

---

## ğŸ“ˆ Performances

### MÃ©triques Globales

| MÃ©trique | Valeur | InterprÃ©tation |
|----------|--------|----------------|
| **F1 Score** | 0.85 | Excellent Ã©quilibre prÃ©cision/recall |
| **Precision** | 0.82 | 82% des "dÃ©parts" prÃ©dits sont corrects |
| **Recall** | 0.88 | 88% des vrais dÃ©parts sont dÃ©tectÃ©s |
| **ROC AUC** | 0.91 | Excellente capacitÃ© de discrimination |
| **Accuracy** | 0.85 | 85% de prÃ©dictions correctes |

### Matrice de Confusion

```
                PrÃ©diction
            Reste     Part
RÃ©alitÃ©
Reste       220       30        (88% spÃ©cificitÃ©)
Part         36      264        (88% recall)
```

**Analyse** :
- **Faux positifs** (30) : EmployÃ©s fidÃ¨les identifiÃ©s Ã  risque â†’ Attention inutile
- **Faux nÃ©gatifs** (36) : DÃ©parts non dÃ©tectÃ©s â†’ Perte de talents
- **Trade-off** : PrivilÃ©gie recall (ne pas rater de dÃ©parts)

### Validation CroisÃ©e

- **MÃ©thode** : 5-fold stratified
- **F1 moyen** : 0.85 Â± 0.03
- **Variance** : Faible â†’ modÃ¨le robuste

---

## ğŸ¯ Features Importantes

Top 10 des variables les plus prÃ©dictives :

| Rang | Feature | Impact | Explication |
|------|---------|--------|-------------|
| 1 | `satisfaction_employee_equilibre_pro_perso` | +++++ | Ã‰quilibre vie pro/perso critique |
| 2 | `annees_dans_l_entreprise` | ++++ | Juniors et trÃ¨s seniors Ã  risque |
| 3 | `heure_supplementaires` | ++++ | Heures sup â†’ burnout |
| 4 | `revenu_mensuel` | +++ | Bas salaires â†’ dÃ©part |
| 5 | `satisfaction_moyenne` | +++ | AgrÃ©gat de satisfaction |
| 6 | `age` | +++ | Jeunes et seniors mobiles |
| 7 | `distance_domicile_travail` | ++ | Distance Ã©levÃ©e â†’ insatisfaction |
| 8 | `nb_formations_suivies` | ++ | Peu de formations â†’ stagnation |
| 9 | `note_evaluation_actuelle` | ++ | Mauvaises Ã©valuations â†’ dÃ©part |
| 10 | `revenu_par_anciennete` | ++ | Sous-rÃ©munÃ©ration dÃ©tectÃ©e |

---

## ğŸ”„ Maintenance

### Quand RÃ©-entraÃ®ner ?

| ScÃ©nario | FrÃ©quence | DÃ©clencheur |
|----------|-----------|-------------|
| **Nouveaux donnÃ©es** | Trimestriel | +500 nouvelles entrÃ©es |
| **Drift dÃ©tectÃ©** | ImmÃ©diat | Performance < 0.75 F1 |
| **Changements mÃ©tier** | Ponctuel | Nouvelles variables RH |

### Protocole de RÃ©-entraÃ®nement

1. Ajouter nouvelles donnÃ©es dans `data/`
2. Lancer `python ml_model/main.py`
3. Comparer dans MLflow : F1, Precision, Recall
4. Valider sur test set
5. Si F1 > ancien modÃ¨le â†’ uploader sur HuggingFace

```bash
# RÃ©-entraÃ®ner
poetry run python ml_model/main.py

# Comparer dans MLflow
mlflow ui --backend-store-uri sqlite:///mlflow.db

# Uploader nouveau modÃ¨le
poetry run python -c "
from huggingface_hub import HfApi
api = HfApi()
api.upload_file(
    path_or_fileobj='model.pkl',
    path_in_repo='model/model.pkl',
    repo_id='ASI-Engineer/employee-turnover-model'
)
"
```

### Monitoring en Production

**MÃ©triques Ã  surveiller** :
- Distribution des prÃ©dictions (% Oui/Non)
- ProbabilitÃ©s moyennes
- Taux de requÃªtes 422 (validation failed)

**Alerte si** :
- PrÃ©dictions "Oui" > 40% (vs 21% attendu)
- ProbabilitÃ©s moyennes < 0.3 ou > 0.7
- Taux d'erreurs 422 > 5%

---

## ğŸ“Š Dataset

| CaractÃ©ristique | Valeur |
|-----------------|--------|
| **Taille totale** | 1470 employÃ©s |
| **Features brutes** | 29 colonnes |
| **Features aprÃ¨s encoding** | 45 (aprÃ¨s OneHot) |
| **Classe cible** | `a_quitte_l_entreprise` (Oui/Non) |
| **DÃ©sÃ©quilibre initial** | 79% Reste / 21% Part |
| **DÃ©sÃ©quilibre aprÃ¨s SMOTE** | 50% / 50% (train uniquement) |
| **Split** | 80% train / 20% test (stratifiÃ©) |

---

## ğŸ”— Liens Utiles

- [EntraÃ®nement](training.md)
- [API](api.md)
- [Installation](installation.md)
