# üéì Entra√Ænement du Mod√®le

Guide pour entra√Æner ou r√©-entra√Æner le mod√®le de pr√©diction du turnover.

---

## üìã Vue d'Ensemble

**Pipeline** : Preprocessing ‚Üí SMOTE ‚Üí XGBoost ‚Üí MLflow  
**Dur√©e** : ~10-15 minutes  
**Output** : Mod√®le + encoders + m√©triques

---

## üöÄ Lancement

```bash
# Avec les donn√©es par d√©faut
poetry run python ml_model/main.py

# Le script encha√Æne automatiquement :
# 1. Chargement et pr√©processing
# 2. Training XGBoost avec RandomizedSearchCV
# 3. Logging dans MLflow
# 4. Sauvegarde des artifacts
```

---

## üìÅ Donn√©es Requises

3 fichiers CSV dans `data/` :

| Fichier | Description | Colonnes Cl√©s |
|---------|-------------|---------------|
| `extrait_sondage.csv` | Sondage satisfaction | `code_sondage`, `a_quitte_l_entreprise`, satisfactions |
| `extrait_eval.csv` | √âvaluations performance | `eval_number`, notes, heures sup, promotions |
| `extrait_sirh.csv` | Donn√©es administratives | `id_employee`, √¢ge, salaire, anciennet√© |

### Format Attendu

**Sondage** :
- `code_sondage`, `a_quitte_l_entreprise` (cible : Oui/Non)
- `nombre_participation_pee`, `nb_formations_suivies`
- `distance_domicile_travail`, `niveau_education`
- `domaine_etude`, `ayant_enfants`, `frequence_deplacement`

**√âvaluation** :
- `eval_number`, `satisfaction_employee_environnement`
- `satisfaction_employee_nature_travail`, `satisfaction_employee_equilibre_pro_perso`
- `satisfaction_employee_relation_hierarchique`
- `note_evaluation_precedente`, `note_evaluation_actuelle`
- `heure_supplementaires`, `augementation_salaire_precedente`

**SIRH** :
- `id_employee`, `age`, `genre`, `revenu_mensuel`
- `statut_marital`, `departement`, `poste`
- `annees_dans_l_entreprise`, `annee_experience_totale`

---

## üîÑ Pipeline de Preprocessing

Le fichier `ml_model/preprocess.py` effectue :

### 1. Chargement et Fusion

```python
# Merge des 3 CSV sur employee_id
df = pd.merge(sondage, eval, on='code')
df = pd.merge(df, sirh, on='id')
```

### 2. Nettoyage

```python
# Parse augmentation salaire : "11 %" ‚Üí 11.0
df['augementation_salaire'] = df['augementation_salaire'].str.replace('%', '').astype(float)

# Winsorize outliers (1% de chaque c√¥t√©)
from scipy.stats.mstats import winsorize
df['revenu'] = winsorize(df['revenu'], limits=[0.01, 0.01])
```

### 3. Feature Engineering

```python
# Ratios normalis√©s par anciennet√©
df['revenu_par_anciennete'] = df['revenu'] / (df['anciennete'] + 1)
df['experience_par_anciennete'] = df['exp_totale'] / (df['anciennete'] + 1)
df['promo_par_anciennete'] = df['annees_promo'] / (df['anciennete'] + 1)

# Agr√©gat satisfaction
satisfaction_cols = [
    'satisfaction_employee_environnement',
    'satisfaction_employee_nature_travail',
    'satisfaction_employee_equilibre_pro_perso',
    'satisfaction_employee_relation_hierarchique'
]
df['satisfaction_moyenne'] = df[satisfaction_cols].mean(axis=1)
```

### 4. Encoding

```python
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

# OneHot : variables non-ordonn√©es
onehot = OneHotEncoder(handle_unknown='ignore')
onehot.fit_transform(df[['genre', 'statut_marital', 'departement', 'poste', 'domaine_etude']])

# Ordinal : fr√©quence d√©placement
ordinal = OrdinalEncoder(categories=[['Aucun', 'Occasionnel', 'Frequent']])
ordinal.fit_transform(df[['frequence_deplacement']])
```

### 5. Scaling

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

---

## üèãÔ∏è Entra√Ænement

Le fichier `ml_model/train_model.py` :

### Configuration

```python
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV

# Pipeline avec SMOTE
pipeline = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('xgb', XGBClassifier(random_state=42))
])

# Hyperparam√®tres √† tester
param_grid = {
    'xgb__n_estimators': [100, 200, 300, 500, 1000],
    'xgb__max_depth': [3, 5, 7, 10, 15],
    'xgb__learning_rate': [0.001, 0.01, 0.1, 0.2, 0.5],
    'xgb__subsample': [0.4, 0.6, 0.8, 0.9, 1.0],
    'xgb__colsample_bytree': [0.5, 0.7, 0.9, 1.0],
    'xgb__reg_alpha': [0, 0.5, 1, 2, 3],
    'xgb__gamma': [0, 1, 2, 5, 10]
}

# RandomizedSearchCV
search = RandomizedSearchCV(
    pipeline,
    param_distributions=param_grid,
    n_iter=1000,
    cv=5,
    scoring='f1',
    random_state=42,
    n_jobs=-1,
    verbose=2
)

# Entra√Ænement
search.fit(X_train, y_train)
```

### Cross-Validation

```python
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(
    pipeline,
    X_train,
    y_train,
    cv=5,
    scoring='f1'
)
print(f"F1 CV: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
```

---

## üìä MLflow Tracking

### Configuration

```python
import mlflow

mlflow.set_tracking_uri("sqlite:///mlflow.db")
mlflow.set_experiment("Employee_Turnover_Training")
```

### Logging

```python
with mlflow.start_run():
    # Param√®tres
    mlflow.log_params(best_params)
    
    # M√©triques
    mlflow.log_metric("f1_score", f1)
    mlflow.log_metric("precision", precision)
    mlflow.log_metric("recall", recall)
    mlflow.log_metric("roc_auc", roc_auc)
    
    # Artifacts
    mlflow.log_artifact("model.pkl")
    mlflow.log_artifact("scaler.joblib")
    mlflow.log_artifact("onehot_encoder.joblib")
    mlflow.log_artifact("ordinal_encoder.joblib")
    
    # Mod√®le
    mlflow.sklearn.log_model(model, "model")
```

### Visualiser les R√©sultats

```bash
# D√©marrer l'UI MLflow
mlflow ui --backend-store-uri sqlite:///mlflow.db

# Ouvrir http://localhost:5000
```

**Interface MLflow** :
- Liste des runs avec m√©triques
- Comparaison de param√®tres
- Visualisation des artifacts
- T√©l√©chargement des mod√®les

---

## üì¶ Artifacts Sauvegard√©s

| Fichier | Description | Utilisation |
|---------|-------------|-------------|
| `model.pkl` | Mod√®le XGBoost entra√Æn√© | Pr√©dictions API |
| `scaler.joblib` | StandardScaler fitt√© | Preprocessing API |
| `onehot_encoder.joblib` | OneHotEncoder fitt√© | Encoding API |
| `ordinal_encoder.joblib` | OrdinalEncoder fitt√© | Encoding API |

---

## üöÄ D√©ployer le Nouveau Mod√®le

### 1. Uploader sur HuggingFace Hub

```python
from huggingface_hub import HfApi

api = HfApi()
api.upload_file(
    path_or_fileobj="model.pkl",
    path_in_repo="model/model.pkl",
    repo_id="ASI-Engineer/employee-turnover-model",
    repo_type="model"
)
```

### 2. L'API Charge Automatiquement

Au red√©marrage, l'API t√©l√©charge le mod√®le depuis HuggingFace Hub.

```python
# Dans src/models.py
from huggingface_hub import hf_hub_download

model_path = hf_hub_download(
    repo_id="ASI-Engineer/employee-turnover-model",
    filename="model/model.pkl"
)
model = joblib.load(model_path)
```

---

## üìà R√©sultats Actuels

| M√©trique | Valeur | CV (5-fold) |
|----------|--------|-------------|
| **F1 Score** | 0.85 | 0.85 ¬± 0.03 |
| **Precision** | 0.82 | 0.82 ¬± 0.04 |
| **Recall** | 0.88 | 0.88 ¬± 0.03 |
| **ROC AUC** | 0.91 | 0.91 ¬± 0.02 |

---

## üîÑ Workflow de R√©-entra√Ænement

### 1. Ajouter Nouvelles Donn√©es

```bash
# Placer les nouveaux CSV dans data/
cp new_sondage.csv data/extrait_sondage.csv
cp new_eval.csv data/extrait_eval.csv
cp new_sirh.csv data/extrait_sirh.csv
```

### 2. R√©-entra√Æner

```bash
poetry run python ml_model/main.py
```

### 3. Comparer dans MLflow

```bash
mlflow ui --backend-store-uri sqlite:///mlflow.db
```

**V√©rifier** :
- F1 Score : am√©lioration ?
- Recall : maintenu > 0.85 ?
- ROC AUC : maintenu > 0.90 ?

### 4. Valider sur Test Set

```python
from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
```

### 5. D√©ployer si Meilleur

```bash
# Uploader sur HuggingFace
poetry run python -c "
from huggingface_hub import HfApi
api = HfApi()
api.upload_file(
    path_or_fileobj='model.pkl',
    path_in_repo='model/model.pkl',
    repo_id='ASI-Engineer/employee-turnover-model'
)
"

# Red√©marrer l'API
# Le nouveau mod√®le sera charg√© automatiquement
```

---

## üîß Optimisation Avanc√©e

### Tuning Manuel

Modifier `ml_model/train_model.py` :

```python
# Augmenter les it√©rations RandomizedSearchCV
n_iter=2000  # au lieu de 1000

# Ajuster la plage des hyperparam√®tres
'xgb__max_depth': [5, 7, 10, 12, 15, 20],
'xgb__learning_rate': [0.005, 0.01, 0.05, 0.1, 0.15],
```

### Features Suppl√©mentaires

Ajouter dans `ml_model/preprocess.py` :

```python
# Exemples d'id√©es
df['taux_augmentation'] = df['augementation_salaire'] / df['revenu']
df['ecart_evaluations'] = df['note_actuelle'] - df['note_precedente']
df['ratio_formations_anciennete'] = df['nb_formations'] / (df['anciennete'] + 1)
```

---

## üîó Liens Utiles

- [Mod√®le ML](model.md)
- [API](api.md)
- [D√©ploiement](deployment.md)
