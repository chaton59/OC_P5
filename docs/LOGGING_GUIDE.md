# üéØ Guide de Logging - Employee Turnover API

## üìã Vue d'ensemble

Le syst√®me de logging utilise **python-json-logger** pour produire des logs structur√©s en JSON, facilitant l'analyse et l'int√©gration avec des outils de monitoring.

## üìÅ Structure des logs

```
logs/
‚îú‚îÄ‚îÄ api.log       # Tous les logs (INFO, WARNING, ERROR)
‚îî‚îÄ‚îÄ error.log     # Erreurs uniquement (ERROR, CRITICAL)
```

## üîß Configuration

### Fichier .env
```bash
# Niveau de log : DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# En mode DEBUG, logs console en format texte
DEBUG=true
```

### Niveaux de log

| Niveau | Usage | Exemple |
|--------|-------|---------|
| **DEBUG** | D√©tails techniques | Variables, √©tats internes |
| **INFO** | √âv√©nements normaux | Requ√™tes, pr√©dictions |
| **WARNING** | Situations anormales non-bloquantes | 404, validation errors |
| **ERROR** | Erreurs n√©cessitant attention | 500, exceptions |
| **CRITICAL** | D√©faillances syst√®me | Mod√®le indisponible |

## üìä Formats de logs

### Log de requ√™te
```json
{
  "timestamp": "2025-12-26T10:30:45.123456",
  "level": "INFO",
  "logger": "employee_turnover_api",
  "module": "app",
  "function": "log_requests",
  "line": 67,
  "message": "Request POST /predict",
  "method": "POST",
  "path": "/predict",
  "status_code": 200,
  "duration_ms": 23.45,
  "client_host": "127.0.0.1"
}
```

### Log de pr√©diction
```json
{
  "timestamp": "2025-12-26T10:30:45.234567",
  "level": "INFO",
  "message": "Prediction made",
  "employee_id": null,
  "prediction": 0,
  "probability": 0.1523,
  "risk_level": "Low",
  "duration_ms": 18.32
}
```

### Log d'erreur
```json
{
  "timestamp": "2025-12-26T10:30:45.345678",
  "level": "ERROR",
  "message": "Unexpected error during prediction",
  "module": "app",
  "function": "predict",
  "line": 215,
  "exc_info": "Traceback (most recent call last):\n..."
}
```

## üîç Utilisation

### Dans le code

```python
from src.logger import logger, log_prediction, log_request

# Log simple
logger.info("Mod√®le charg√©", extra={"version": "2.1.0"})

# Log avec m√©tadonn√©es
logger.warning("Pr√©diction lente", extra={
    "duration_ms": 500,
    "employee_id": "EMP123"
})

# Log d'erreur avec exception
try:
    result = risky_operation()
except Exception as e:
    logger.exception("Operation failed")  # Inclut traceback
```

### Fonctions utilitaires

```python
# Logger une requ√™te HTTP
log_request(
    method="POST",
    path="/predict",
    status_code=200,
    duration_ms=23.45,
    user_id="user123"  # M√©tadonn√©es custom
)

# Logger une pr√©diction
log_prediction(
    employee_id="EMP123",
    prediction=1,
    probability=0.87,
    risk_level="high",
    duration_ms=18.5
)

# Logger chargement du mod√®le
log_model_load(
    model_type="XGBoost Pipeline",
    duration_ms=1234.5,
    success=True
)
```

## üìà Analyse des logs

### Commandes bash

```bash
# Suivre les logs en temps r√©el
tail -f logs/api.log

# Filtrer par niveau
cat logs/api.log | jq 'select(.level=="ERROR")'

# Requ√™tes les plus lentes
cat logs/api.log | jq 'select(.path=="/predict") | .duration_ms' | sort -n | tail -10

# Nombre d'erreurs par endpoint
cat logs/error.log | jq -r '.path' | sort | uniq -c

# Pr√©dictions par risk level
cat logs/api.log | jq 'select(.risk_level != null) | .risk_level' | sort | uniq -c

# Temps moyen de pr√©diction
cat logs/api.log | jq 'select(.message=="Prediction made") | .duration_ms' | jq -s 'add/length'

# Top 10 IPs
cat logs/api.log | jq -r '.client_host' | sort | uniq -c | sort -rn | head -10
```

### Requ√™tes jq avanc√©es

```bash
# Erreurs avec contexte
cat logs/error.log | jq '{time: .timestamp, error: .message, module: .module, line: .line}'

# Stats par status code
cat logs/api.log | jq -r 'select(.status_code) | .status_code' | sort | uniq -c

# Pr√©dictions high risk
cat logs/api.log | jq 'select(.risk_level=="high") | {time: .timestamp, prob: .probability}'

# D√©tection d'anomalies (>1s)
cat logs/api.log | jq 'select(.duration_ms > 1000) | {path: .path, duration: .duration_ms}'
```

## üéõÔ∏è Int√©gration monitoring

### ELK Stack (Elasticsearch + Logstash + Kibana)

**Logstash config** :
```ruby
input {
  file {
    path => "/app/logs/api.log"
    codec => "json"
    type => "api_logs"
  }
}

filter {
  if [type] == "api_logs" {
    date {
      match => ["timestamp", "ISO8601"]
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "api-logs-%{+YYYY.MM.dd}"
  }
}
```

### Grafana Loki

**Promtail config** :
```yaml
clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: api_logs
    static_configs:
      - targets: [localhost]
        labels:
          job: employee_turnover_api
          __path__: /app/logs/api.log
    pipeline_stages:
      - json:
          expressions:
            level: level
            message: message
            status_code: status_code
```

### CloudWatch (AWS)

```python
import watchtower
import logging

logger = logging.getLogger("employee_turnover_api")
logger.addHandler(watchtower.CloudWatchLogHandler(
    log_group="/aws/api/employee-turnover",
    stream_name="production"
))
```

## üö® Alertes recommand√©es

### 1. Taux d'erreur √©lev√©
```
Condition: (errors / total_requests) > 0.05
Action: Email + PagerDuty
```

### 2. Latence √©lev√©e
```
Condition: avg(duration_ms) > 500
Action: Slack notification
```

### 3. Rate limiting d√©clench√©
```
Condition: count(status_code==429) > 10
Action: Log alert
```

### 4. Mod√®le non disponible
```
Condition: log_message contains "Model not available"
Action: Critical alert + SMS
```

## üìù Best practices

### ‚úÖ √Ä faire

```python
# Logs avec contexte
logger.info("Processing request", extra={
    "user_id": user_id,
    "endpoint": "/predict",
    "payload_size": len(data)
})

# Utiliser les niveaux appropri√©s
logger.debug("Variable value", extra={"x": x})  # Dev only
logger.info("User action", extra={"action": "predict"})  # Normal
logger.warning("Slow query", extra={"duration": 2.5})  # Attention
logger.error("Failed", exc_info=True)  # Erreur
```

### ‚ùå √Ä √©viter

```python
# Logs sans contexte
logger.info("Error")  # Quoi ? O√π ? Pourquoi ?

# Donn√©es sensibles
logger.info(f"API Key: {api_key}")  # JAMAIS !

# Logs excessifs en boucle
for item in large_list:
    logger.debug(f"Processing {item}")  # Pollue les logs
```

## üîê S√©curit√©

### Donn√©es √† masquer
- API Keys
- Tokens d'authentification
- Informations personnelles (PII)
- Mots de passe
- Emails complets

### Exemple de masquage
```python
def mask_sensitive(data):
    if "api_key" in data:
        data["api_key"] = data["api_key"][:8] + "***"
    if "email" in data:
        data["email"] = data["email"].split("@")[0] + "@***"
    return data

logger.info("User data", extra=mask_sensitive(user_data))
```

## üìä M√©triques utiles

### Performance
- `duration_ms` : Temps de traitement
- `status_code` : Codes HTTP
- `model_load_time` : Temps chargement mod√®le

### Business
- `prediction` : 0 ou 1
- `risk_level` : Low, Medium, High
- `probability` : Probabilit√© de turnover

### Syst√®me
- `level` : Niveau de log
- `client_host` : IP du client
- `path` : Endpoint appel√©

## üîÑ Rotation des logs

**Configuration recommand√©e** :
```python
from logging.handlers import RotatingFileHandler

handler = RotatingFileHandler(
    "logs/api.log",
    maxBytes=10*1024*1024,  # 10 MB
    backupCount=5            # Garder 5 fichiers
)
```

**Avec logrotate** :
```bash
# /etc/logrotate.d/employee-turnover-api
/app/logs/*.log {
    daily
    rotate 7
    compress
    delaycompress
    notifempty
    missingok
}
```

## üìö Ressources

- [python-json-logger](https://github.com/madzak/python-json-logger)
- [ELK Stack](https://www.elastic.co/elk-stack)
- [Grafana Loki](https://grafana.com/oss/loki/)
- [AWS CloudWatch](https://aws.amazon.com/cloudwatch/)

---

**Derni√®re mise √† jour** : 26 d√©cembre 2025  
**Version** : 2.1.0
