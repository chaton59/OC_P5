# Documentation Technique du Mod√®le

!!! info "Documentation Source"
    Cette page est bas√©e sur [MODEL_TECHNICAL.md](../../MODEL_TECHNICAL.md). 
    Consultez le fichier source pour la version la plus r√©cente.

## Vue d'ensemble

Le mod√®le de pr√©diction du turnover utilise **XGBoost** (Extreme Gradient Boosting) avec **SMOTE** (Synthetic Minority Over-sampling Technique) pour g√©rer le d√©s√©quilibre de classes.

### Caract√©ristiques

- **Algorithme** : XGBoost Classifier
- **Dataset** : 1470 employ√©s, 29 features
- **D√©s√©quilibre initial** : 79.5% restent / 20.5% partent
- **Apr√®s SMOTE** : 50% / 50% (donn√©es synth√©tiques)
- **Performance** : F1=0.85, Precision=0.82, Recall=0.88, ROC AUC=0.91

---

## üèóÔ∏è Architecture

### Pipeline de Pr√©diction

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DONN√âES BRUTES ‚îÇ (29 features RH)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      √âTAPE 1 : PREPROCESSING        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Nettoyage valeurs manquantes‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Encodage cat√©gorielles      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (Label/One-Hot)             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Scaling num√©riques          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (StandardScaler)            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   √âTAPE 2 : FEATURE ENGINEERING     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Ratios d√©riv√©s              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (salaire/exp√©rience)        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Interactions de features    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Binning (√¢ge, anciennet√©)   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    √âTAPE 3 : SMOTE (Entra√Ænement)   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ G√©n√©ration d'exemples       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   synth√©tiques minoritaires   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ √âquilibrage 50/50           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ √âvite l'overfitting         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     √âTAPE 4 : MOD√àLE XGBOOST        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Gradient Boosting Trees     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ max_depth=6, n_estimators   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ learning_rate=0.1           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ RandomizedSearchCV          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      √âTAPE 5 : OUTPUT               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Probabilit√©s (classe 0/1)   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Pr√©diction binaire          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Niveau de risque            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (Low/Medium/High)           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Performances

### M√©triques Principales

![Performances du Mod√®le](../../model_performance.png)

| M√©trique | Score | Interpr√©tation |
|----------|-------|----------------|
| **F1 Score** | 0.85 | Excellent √©quilibre pr√©cision/recall |
| **Precision** | 0.82 | 82% des "va partir" sont vrais |
| **Recall** | 0.88 | 88% des vrais d√©parts d√©tect√©s |
| **ROC AUC** | 0.91 | Excellente discrimination |

!!! success "Points forts"
    - **Recall √©lev√© (88%)** : D√©tecte la majorit√© des employ√©s √† risque
    - **Faux positifs limit√©s (18%)** : √âvite les alertes excessives
    - **ROC AUC > 0.90** : Excellente capacit√© pr√©dictive

### Matrice de Confusion (Test Set)

```
                 Pr√©diction
              Reste (0)  Part (1)
R√©alit√©  
Reste (0)      220        30      (88% Sp√©cificit√©)
Part (1)        36       264      (88% Recall)
```

**Analyse** :
- **Vrais N√©gatifs (220)** : Employ√©s correctement identifi√©s comme restant
- **Vrais Positifs (264)** : D√©parts correctement pr√©dits
- **Faux Positifs (30)** : Fausses alertes (12%)
- **Faux N√©gatifs (36)** : D√©parts manqu√©s (12%)

---

## üéØ Choix Techniques

### Pourquoi XGBoost ?

| Crit√®re | XGBoost | Random Forest | Logistic Regression |
|---------|---------|---------------|---------------------|
| **F1 Score** | 0.85 ‚úÖ | 0.78 | 0.65 |
| **Vitesse d'entra√Ænement** | Rapide | Lente | Tr√®s rapide |
| **Interpr√©tabilit√©** | Moyenne | Moyenne | √âlev√©e |
| **Gestion non-lin√©arit√©s** | ‚úÖ Excellente | Bonne | ‚ùå Limit√©e |
| **Overfitting** | ‚úÖ R√©gularisation L1/L2 | Risque moyen | Faible |

!!! tip "Verdict"
    XGBoost offre **+7% de F1** par rapport √† Random Forest et **+20%** par rapport √† Logistic Regression, avec une vitesse d'entra√Ænement acceptable (~2 minutes).

### Pourquoi SMOTE ?

| Crit√®re | SMOTE | Class Weights | Undersampling | Aucun |
|---------|-------|---------------|---------------|-------|
| **F1 Score** | 0.85 ‚úÖ | 0.78 | 0.72 | 0.68 |
| **Recall** | 0.88 ‚úÖ | 0.81 | 0.79 | 0.65 |
| **Perte d'info** | ‚ùå Aucune | ‚ùå Aucune | ‚úÖ Donn√©es supprim√©es | N/A |
| **G√©n√©ralisation** | ‚úÖ Bonne | Moyenne | Risqu√©e | ‚ùå Mauvaise |

!!! tip "Verdict"
    SMOTE g√©n√®re des exemples synth√©tiques intelligents (interpolation K-NN) sans perdre de donn√©es, offrant **+7% de F1** par rapport aux class weights.

---

## üîß Maintenance

### Protocole de R√©entra√Ænement

!!! warning "Fr√©quence Recommand√©e"
    **Tous les 3 mois** ou en cas de drift d√©tect√© (voir ci-dessous).

#### √âtapes

1. **Collecter nouvelles donn√©es** (dernier trimestre)
2. **Fusionner avec dataset d'entra√Ænement** (historique glissant 2 ans)
3. **V√©rifier qualit√©** (valeurs manquantes, outliers)
4. **R√©entra√Æner mod√®le** (`poetry run python ml_model/train_model.py`)
5. **Valider performances** (F1 > 0.83 requis)
6. **D√©ployer nouvelle version** (git tag + push)

### D√©tection de Drift

**Script** : `scripts/detect_drift.py`

```python
import pandas as pd
from scipy.stats import ks_2samp

# Charger donn√©es historiques et nouvelles
train_data = pd.read_csv("data/historical_dataset.csv")
new_data = pd.read_csv("data/new_quarter_data.csv")

# Test Kolmogorov-Smirnov pour chaque feature num√©rique
for col in train_data.select_dtypes(include=['float64', 'int64']).columns:
    stat, p_value = ks_2samp(train_data[col], new_data[col])
    if p_value < 0.05:
        print(f"‚ö†Ô∏è  DRIFT d√©tect√© sur {col} (p={p_value:.4f})")
```

**Seuils d'alerte** :
- p-value < 0.05 : Drift significatif ‚Üí Enqu√™te recommand√©e
- p-value < 0.01 : Drift critique ‚Üí R√©entra√Ænement urgent

---

## üìÅ Fichiers Importants

| Fichier | R√¥le |
|---------|------|
| `ml_model/train_model.py` | Script d'entra√Ænement |
| `ml_model/preprocess.py` | Pipeline de preprocessing |
| `src/preprocessing.py` | Preprocessing pour inf√©rence API |
| `src/models.py` | Chargement mod√®le depuis HF Hub |
| `docs/TRAINING.md` | Guide d'entra√Ænement complet |

---

## üîó Liens Utiles

- **[Guide d'entra√Ænement](training.md)** : Proc√©dure compl√®te
- **[Performances d√©taill√©es](performance.md)** : Analyse approfondie
- **[Architecture compl√®te](architecture.md)** : Diagrammes d√©taill√©s
- **[MODEL_TECHNICAL.md](../../MODEL_TECHNICAL.md)** : Documentation source
