#!/usr/bin/env python3
"""
Module de chargement et gestion du mod√®le MLflow.

Ce module encapsule la logique de chargement du mod√®le depuis Hugging Face Hub
via MLflow, avec gestion des erreurs et versioning.
"""
from typing import Any, Optional

from fastapi import HTTPException
from huggingface_hub import hf_hub_download

# Configuration
HF_MODEL_REPO = "ASI-Engineer/employee-turnover-model"
MODEL_FILENAME = "model/model.pkl"

# Cache global du mod√®le
_model_cache: Optional[Any] = None


def load_model(force_reload: bool = False) -> Any:
    """
    Charge le mod√®le depuis Hugging Face Hub via MLflow.

    Cette fonction impl√©mente un syst√®me de cache pour √©viter de recharger
    le mod√®le √† chaque appel. Le mod√®le est charg√© une seule fois au d√©marrage
    de l'application et mis en cache.

    Args:
        force_reload: Si True, force le rechargement du mod√®le m√™me s'il est en cache.

    Returns:
        Le mod√®le MLflow charg√© et pr√™t pour l'inf√©rence.

    Raises:
        HTTPException: 500 si le mod√®le ne peut pas √™tre charg√©.

    Examples:
        >>> model = load_model()
        >>> # Utiliser le mod√®le pour pr√©diction
        >>> predictions = model.predict(X)
    """
    global _model_cache

    # Retourner le mod√®le en cache si disponible
    if _model_cache is not None and not force_reload:
        return _model_cache

    try:
        import joblib
        import logging

        logger = logging.getLogger(__name__)

        logger.info(f"üîÑ Chargement du mod√®le depuis HF Hub: {HF_MODEL_REPO}")

        # T√©l√©charger le mod√®le depuis Hugging Face Hub avec timeout
        try:
            model_path = hf_hub_download(
                repo_id=HF_MODEL_REPO,
                filename=MODEL_FILENAME,
                repo_type="model",
                timeout=60,  # Timeout de 60 secondes
            )
        except Exception as download_error:
            logger.error(f"Erreur t√©l√©chargement HF Hub: {download_error}")
            raise

        logger.info(f"üì¶ Mod√®le t√©l√©charg√©: {model_path}")

        # Charger le mod√®le avec joblib
        model = joblib.load(model_path)

        # Mettre en cache
        _model_cache = model

        logger.info(f"‚úÖ Mod√®le charg√© avec succ√®s: {type(model).__name__}")
        return model

    except Exception as e:
        import logging

        logger = logging.getLogger(__name__)
        error_msg = f"‚ùå Erreur lors du chargement du mod√®le: {str(e)}"
        logger.error(error_msg)
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Model loading failed",
                "message": str(e),
                "model_repo": HF_MODEL_REPO,
                "solution": "V√©rifiez que le mod√®le est disponible sur HF Hub et correctement entra√Æn√©",
            },
        )


def get_model_info() -> dict:
    """
    Retourne les informations sur le mod√®le charg√©.

    Returns:
        Dict contenant les m√©tadonn√©es du mod√®le.

    Raises:
        HTTPException: 500 si le mod√®le n'est pas charg√©.
    """
    try:
        model = load_model()

        return {
            "status": "‚úÖ Mod√®le charg√©",
            "model_type": type(model).__name__,
            "hf_hub_repo": HF_MODEL_REPO,
            "model_file": MODEL_FILENAME,
            "cached": _model_cache is not None,
        }

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail={"error": "Model info unavailable", "message": str(e)},
        )


def load_preprocessing_artifacts(run_id: str) -> dict:
    """
    Charge les artifacts de preprocessing (scaler, encoders) depuis MLflow.

    Args:
        run_id: ID du run MLflow contenant les artifacts.

    Returns:
        Dict contenant les artifacts de preprocessing.

    Raises:
        HTTPException: 500 si les artifacts ne peuvent pas √™tre charg√©s.

    Note:
        Cette fonction sera impl√©ment√©e quand les preprocessing artifacts
        seront disponibles dans le mod√®le HF Hub.
    """
    raise NotImplementedError(
        "Le chargement des preprocessing artifacts sera impl√©ment√© "
        "lors de l'int√©gration compl√®te avec MLflow"
    )


if __name__ == "__main__":
    # Test de chargement du mod√®le
    print("=" * 80)
    print("TEST DE CHARGEMENT DU MOD√àLE")
    print("=" * 80)

    try:
        model = load_model()
        print("\n‚úÖ Test r√©ussi!")
        print(f"Type de mod√®le: {type(model).__name__}")

        # Afficher les infos
        info = get_model_info()
        print("\nInformations du mod√®le:")
        for key, value in info.items():
            print(f"  {key}: {value}")

    except Exception as e:
        print(f"\n‚ùå Test √©chou√©: {e}")
