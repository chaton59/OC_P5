#!/usr/bin/env python3
"""
Pipeline principal d'entra√Ænement du mod√®le Employee Turnover.

Ce script encha√Æne:
1. Chargement et pr√©processing des donn√©es
2. Entra√Ænement du mod√®le XGBoost avec RandomizedSearchCV et SMOTE
3. Logging des r√©sultats dans MLflow (params, metrics, artifacts, model)
4. Sauvegarde des encoders et scaler pour utilisation future

Usage:
    python main.py

Le mod√®le et les artifacts sont enregistr√©s dans MLflow pour:
- Suivi des exp√©rimentations
- Reproductibilit√©
    D√©ploiement via Model Registry
"""
from pathlib import Path

import joblib
import mlflow
import mlflow.sklearn
from ml_model.preprocess import preprocess_data
from ml_model.train_model import train_model


def main():
    """Pipeline principal d'entra√Ænement."""
    print("=" * 80)
    print("üöÄ PIPELINE D'ENTRA√éNEMENT - Employee Turnover Prediction")
    print("=" * 80)
    print()

    # Configuration MLflow
    mlflow.set_tracking_uri("sqlite:///mlflow.db")
    mlflow.set_experiment("Employee_Turnover_Training")

    print("üìä Configuration MLflow:")
    print(f"   Tracking URI: {mlflow.get_tracking_uri()}")
    print("   Experiment: Employee_Turnover_Training")
    print()

    # Chemins des donn√©es
    data_paths = {
        "sondage_path": "data/extrait_sondage.csv",
        "eval_path": "data/extrait_eval.csv",
        "sirh_path": "data/extrait_sirh.csv",
    }

    # V√©rifier que les fichiers existent
    for name, path in data_paths.items():
        if not Path(path).exists():
            raise FileNotFoundError(f"‚ùå Fichier manquant: {path}")

    print("‚úÖ Fichiers de donn√©es trouv√©s")
    print()

    # ========================================================================
    # √âTAPE 1 : Pr√©processing
    # ========================================================================
    print("1Ô∏è‚É£  PR√âPROCESSING")
    print("-" * 80)

    X, y, scaler, onehot_encoder, ordinal_encoder = preprocess_data(data_paths)

    print(f"   Forme X: {X.shape}")
    print(f"   Forme y: {y.shape}")
    print(f"   Classes: {y.value_counts().to_dict()}")
    print(f"   Ratio d√©s√©quilibre: {(y == 0).sum() / (y == 1).sum():.2f}:1")
    print()

    # ========================================================================
    # √âTAPE 2 : Entra√Ænement avec MLflow tracking
    # ========================================================================
    print("2Ô∏è‚É£  ENTRA√éNEMENT")
    print("-" * 80)

    # Entra√Ænement (d√©j√† avec MLflow tracking dans train_model.py)
    model, best_params, cv_f1 = train_model(X, y)

    print("   ‚úÖ Mod√®le entra√Æn√©")
    print(f"   üèÜ Meilleur F1 CV: {cv_f1:.4f}")
    print()

    # R√©cup√©rer le run actif pour sauvegarder les artifacts
    active_run = mlflow.active_run()
    if active_run is None:
        # Si train_model a ferm√© le run, on en ouvre un nouveau
        active_run = mlflow.start_run()
        run_id = active_run.info.run_id
        should_end_run = True
    else:
        run_id = active_run.info.run_id
        should_end_run = False

    # Log des infos dataset
    mlflow.log_param("n_samples", len(X))
    mlflow.log_param("n_features", X.shape[1])
    mlflow.log_param("class_ratio", f"{(y == 0).sum()}:{(y == 1).sum()}")

    # ========================================================================
    # √âTAPE 3 : Sauvegarde des artifacts (encoders, scaler)
    # ========================================================================
    print("3Ô∏è‚É£  SAUVEGARDE DES ARTIFACTS")
    print("-" * 80)

    # Cr√©er dossier temporaire pour artifacts
    artifacts_dir = Path("artifacts_temp")
    artifacts_dir.mkdir(exist_ok=True)

    # Sauvegarder scaler
    scaler_path = artifacts_dir / "scaler.joblib"
    joblib.dump(scaler, scaler_path)
    mlflow.log_artifact(str(scaler_path), artifact_path="preprocessing")
    print("   ‚úÖ Scaler sauvegard√©")

    # Sauvegarder encoders (onehot et ordinal)
    onehot_path = artifacts_dir / "onehot_encoder.joblib"
    joblib.dump(onehot_encoder, onehot_path)
    mlflow.log_artifact(str(onehot_path), artifact_path="preprocessing")

    ordinal_path = artifacts_dir / "ordinal_encoder.joblib"
    joblib.dump(ordinal_encoder, ordinal_path)
    mlflow.log_artifact(str(ordinal_path), artifact_path="preprocessing")
    print("   ‚úÖ Encoders sauvegard√©s (OneHot + Ordinal)")

    # Log git commit si disponible
    try:
        import subprocess

        git_commit = (
            subprocess.check_output(["git", "rev-parse", "HEAD"])
            .strip()
            .decode("utf-8")
        )
        mlflow.set_tag("git_commit", git_commit[:8])
        print(f"   ‚úÖ Git commit: {git_commit[:8]}")
    except Exception:
        pass

    # Nettoyer artifacts temporaires
    scaler_path.unlink()
    onehot_path.unlink()
    ordinal_path.unlink()
    artifacts_dir.rmdir()

    print()

    # Fermer le run si on l'a ouvert
    if should_end_run:
        mlflow.end_run()

    # ========================================================================
    # R√âSUM√â
    # ========================================================================
    print("=" * 80)
    print("‚úÖ ENTRA√éNEMENT TERMIN√â")
    print("=" * 80)
    print()
    print(f"üìä Run ID: {run_id}")
    print(f"üéØ F1 Score (CV): {cv_f1:.4f}")
    print("üì¶ Artifacts sauvegard√©s dans MLflow")
    print()
    print("üåê Pour visualiser les r√©sultats:")
    print("   ./scripts/start_mlflow.sh")
    print("   ou: mlflow ui --backend-store-uri sqlite:///mlflow.db")
    print()
    print("üìù Pour charger le mod√®le:")
    print(f"   model = mlflow.sklearn.load_model('runs:/{run_id}/model')")
    print()


if __name__ == "__main__":
    main()
