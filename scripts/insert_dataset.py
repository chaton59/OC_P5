#!/usr/bin/env python3
"""
Script pour insÃ©rer le dataset complet dans PostgreSQL.

Ce script :
1. Lit les fichiers CSV (sondage, eval, sirh)
2. Fusionne les donnÃ©es selon les clÃ©s communes
3. InsÃ¨re dans la table dataset de PostgreSQL
"""
import os
import pandas as pd
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from src.config import get_settings


def load_csv_files():
    """Charge les fichiers CSV."""
    print("ğŸ“‚ Chargement des fichiers CSV...")

    # Chemins des fichiers
    data_dir = "data"
    sondage_file = os.path.join(data_dir, "extrait_sondage.csv")
    eval_file = os.path.join(data_dir, "extrait_eval.csv")
    sirh_file = os.path.join(data_dir, "extrait_sirh.csv")

    # Charger les dataframes
    df_sondage = pd.read_csv(sondage_file)
    df_eval = pd.read_csv(eval_file)
    df_sirh = pd.read_csv(sirh_file)

    print(f"âœ… Sondage: {len(df_sondage)} lignes")
    print(f"âœ… Ã‰valuation: {len(df_eval)} lignes")
    print(f"âœ… SIRH: {len(df_sirh)} lignes")

    return df_sondage, df_eval, df_sirh


def merge_datasets(df_sondage, df_eval, df_sirh):
    """Fusionne les datasets selon les clÃ©s communes."""
    print("ğŸ”— Fusion des datasets...")

    # Les datasets semblent dÃ©jÃ  avoir le mÃªme nombre de lignes et Ãªtre dans le mÃªme ordre
    # On peut les concatÃ©ner horizontalement
    df_merged = pd.concat([df_sondage, df_eval, df_sirh], axis=1)

    # Supprimer les colonnes dupliquÃ©es si elles existent
    df_merged = df_merged.loc[:, ~df_merged.columns.duplicated()]

    print(
        f"âœ… Dataset fusionnÃ©: {len(df_merged)} lignes, {len(df_merged.columns)} colonnes"
    )
    print(f"ğŸ“Š Colonnes: {list(df_merged.columns)}")

    return df_merged


def prepare_for_db(df):
    """PrÃ©pare les donnÃ©es pour l'insertion en base."""
    print("ğŸ”§ PrÃ©paration des donnÃ©es pour la DB...")

    # SÃ©parer les features et la target
    # La colonne 'a_quitte_l_entreprise' semble Ãªtre la target (Oui/Non)
    target_col = "a_quitte_l_entreprise"

    if target_col in df.columns:
        features_df = df.drop(columns=[target_col])
        target_df = df[target_col]
    else:
        print(
            "âš ï¸ Colonne target non trouvÃ©e, utilisation de toutes les colonnes comme features"
        )
        features_df = df
        target_df = pd.Series(["Non"] * len(df))  # Valeur par dÃ©faut

    print(f"âœ… Features: {len(features_df.columns)} colonnes")
    print(f"âœ… Target: {len(target_df)} valeurs")

    return features_df, target_df


def insert_into_db(features_df, target_df, db_url):
    """InsÃ¨re les donnÃ©es dans PostgreSQL."""
    print("ğŸ’¾ Insertion en base de donnÃ©es...")

    # CrÃ©er la connexion
    engine = create_engine(db_url)
    Session = sessionmaker(bind=engine)
    session = Session()

    try:
        # Importer le modÃ¨le
        from db_models import Dataset

        # Vider la table existante (optionnel, pour Ã©viter les doublons)
        session.query(Dataset).delete()
        session.commit()
        print("ğŸ—‘ï¸ Table dataset vidÃ©e")

        # InsÃ©rer les donnÃ©es
        inserted_count = 0
        for idx, row in features_df.iterrows():
            # Convertir la ligne en dict JSON
            features_dict = row.to_dict()

            # Nettoyer les valeurs (remplacer NaN par None)
            features_dict = {
                k: (v if pd.notna(v) else None) for k, v in features_dict.items()
            }

            # RÃ©cupÃ©rer la target
            target = str(target_df.iloc[idx]) if idx < len(target_df) else "Non"

            # CrÃ©er l'enregistrement
            dataset_entry = Dataset(features_json=features_dict, target=target)

            session.add(dataset_entry)
            inserted_count += 1

            # Commit par batch de 100 pour performance
            if inserted_count % 100 == 0:
                session.commit()
                print(f"ğŸ“Š {inserted_count} enregistrements insÃ©rÃ©s...")

        # Commit final
        session.commit()
        print(f"âœ… Insertion terminÃ©e: {inserted_count} enregistrements")

    except Exception as e:
        session.rollback()
        print(f"âŒ Erreur lors de l'insertion: {e}")
        raise
    finally:
        session.close()


def main():
    """Fonction principale."""
    print("ğŸš€ Insertion du dataset complet dans PostgreSQL\n")

    try:
        # Charger la configuration
        settings = get_settings()
        db_url = settings.DATABASE_URL

        # Ã‰tape 1: Charger les CSV
        df_sondage, df_eval, df_sirh = load_csv_files()

        # Ã‰tape 2: Fusionner
        df_merged = merge_datasets(df_sondage, df_eval, df_sirh)

        # Ã‰tape 3: PrÃ©parer pour DB
        features_df, target_df = prepare_for_db(df_merged)

        # Ã‰tape 4: InsÃ©rer en DB
        insert_into_db(features_df, target_df, db_url)

        print("\nğŸ‰ Dataset insÃ©rÃ© avec succÃ¨s !")
        print("ğŸ“Š VÃ©rifiez avec: SELECT COUNT(*) FROM dataset;")

    except Exception as e:
        print(f"\nâŒ Erreur: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
